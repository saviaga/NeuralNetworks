{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AlexNet-like NN.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "6sZBTtmEYrTM",
        "colab_type": "code",
        "outputId": "1e66caec-677d-4768-eab1-a6a42cc13513",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2642
        }
      },
      "cell_type": "code",
      "source": [
        "#This is an AlexNet-like Neural Network trained on CIFAR-10 data set of resolution (32x32.\n",
        "#\n",
        "\n",
        "import keras\n",
        "from keras.datasets import cifar10\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten\n",
        "from keras.layers import Conv2D, MaxPooling2D, BatchNormalization\n",
        "from keras.optimizers import Adam\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Hyperparameters\n",
        "batch_size = 128\n",
        "num_classes = 10\n",
        "epochs = 1000\n",
        "\n",
        "# Load CIFAR10 Data\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "img_height, img_width, channel = x_train.shape[1],x_train.shape[2],x_train.shape[3]\n",
        "\n",
        "# convert to one hot encoing \n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "# AlexNet Define the Model\n",
        "model = Sequential()\n",
        "model.add(Conv2D(96, (11,11), strides=(4,4), activation='relu', padding='same', input_shape=(img_height, img_width, channel,)))\n",
        "# for original Alexnet\n",
        "model.add(Conv2D(48, (3,3), strides=(2,2), activation='relu', padding='same', input_shape=(img_height, img_width, channel,)))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2), strides=(2,2)))\n",
        "# Local Response normalization for Original Alexnet\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(Conv2D(96, (3,3), activation='relu', padding='same'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2), strides=(2,2)))\n",
        "# Local Response normalization for Original Alexnet\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(Conv2D(192, (3,3), activation='relu', padding='same'))\n",
        "model.add(Conv2D(192, (3,3), activation='relu', padding='same'))\n",
        "model.add(Conv2D(256, (3,3), activation='relu', padding='same'))\n",
        "model.add(MaxPooling2D(pool_size=(3, 3), strides=(2,2)))\n",
        "# Local Response normalization for Original Alexnet\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(512, activation='tanh'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(256, activation='tanh'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "\n",
        "# print the model summary\n",
        "model.summary()\n",
        "\n",
        "# determine Loss function and Optimizer\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=Adam(),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train the Model\n",
        "\n",
        "history= model.fit(x_train, y_train,\n",
        "                    batch_size=batch_size,\n",
        "                    epochs=epochs,\n",
        "                    verbose=1,\n",
        "                    validation_data=(x_test, y_test))\n",
        "\n",
        "# Test the model\n",
        "score = model.evaluate(x_test, y_test, verbose=0)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])\n",
        "print(history.history.keys())\n",
        "#  \"Accuracy\"\n",
        "plt.plot(history.history['acc'])\n",
        "plt.plot(history.history['val_acc'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()\n",
        "# summarize history for loss\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 15s 0us/step\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_1 (Conv2D)            (None, 16, 16, 48)        1344      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 8, 8, 48)          0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 8, 8, 48)          192       \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 8, 8, 96)          41568     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 4, 4, 96)          0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 4, 4, 96)          384       \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 4, 4, 192)         166080    \n",
            "_________________________________________________________________\n",
            "conv2d_4 (Conv2D)            (None, 4, 4, 192)         331968    \n",
            "_________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2 (None, 1, 1, 192)         0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 1, 1, 192)         768       \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 192)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 512)               98816     \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 256)               131328    \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 10)                2570      \n",
            "=================================================================\n",
            "Total params: 775,018\n",
            "Trainable params: 774,346\n",
            "Non-trainable params: 672\n",
            "_________________________________________________________________\n",
            "Train on 50000 samples, validate on 10000 samples\n",
            "Epoch 1/1000\n",
            "50000/50000 [==============================] - 17s 336us/step - loss: 1.5385 - acc: 0.4602 - val_loss: 1.3295 - val_acc: 0.5517\n",
            "Epoch 2/1000\n",
            "50000/50000 [==============================] - 13s 261us/step - loss: 1.0607 - acc: 0.6273 - val_loss: 1.2328 - val_acc: 0.5757\n",
            "Epoch 3/1000\n",
            "50000/50000 [==============================] - 14s 278us/step - loss: 0.8552 - acc: 0.7048 - val_loss: 0.9633 - val_acc: 0.6637\n",
            "Epoch 4/1000\n",
            "50000/50000 [==============================] - 14s 278us/step - loss: 0.7269 - acc: 0.7508 - val_loss: 0.9955 - val_acc: 0.6744\n",
            "Epoch 5/1000\n",
            "50000/50000 [==============================] - 14s 278us/step - loss: 0.6280 - acc: 0.7863 - val_loss: 1.0053 - val_acc: 0.6826\n",
            "Epoch 6/1000\n",
            "50000/50000 [==============================] - 14s 277us/step - loss: 0.5302 - acc: 0.8198 - val_loss: 0.9552 - val_acc: 0.7020\n",
            "Epoch 7/1000\n",
            "50000/50000 [==============================] - 14s 276us/step - loss: 0.4548 - acc: 0.8464 - val_loss: 0.9436 - val_acc: 0.7224\n",
            "Epoch 8/1000\n",
            "50000/50000 [==============================] - 14s 276us/step - loss: 0.3685 - acc: 0.8758 - val_loss: 0.9638 - val_acc: 0.7236\n",
            "Epoch 9/1000\n",
            "50000/50000 [==============================] - 14s 276us/step - loss: 0.3194 - acc: 0.8925 - val_loss: 1.2448 - val_acc: 0.6689\n",
            "Epoch 10/1000\n",
            "50000/50000 [==============================] - 14s 276us/step - loss: 0.2733 - acc: 0.9074 - val_loss: 1.0440 - val_acc: 0.7211\n",
            "Epoch 11/1000\n",
            "50000/50000 [==============================] - 14s 274us/step - loss: 0.2250 - acc: 0.9229 - val_loss: 1.2377 - val_acc: 0.7009\n",
            "Epoch 12/1000\n",
            "50000/50000 [==============================] - 14s 276us/step - loss: 0.1921 - acc: 0.9348 - val_loss: 1.2403 - val_acc: 0.7025\n",
            "Epoch 13/1000\n",
            "50000/50000 [==============================] - 14s 277us/step - loss: 0.1751 - acc: 0.9422 - val_loss: 1.1928 - val_acc: 0.7161\n",
            "Epoch 14/1000\n",
            "50000/50000 [==============================] - 14s 277us/step - loss: 0.1593 - acc: 0.9455 - val_loss: 1.3472 - val_acc: 0.6913\n",
            "Epoch 15/1000\n",
            "50000/50000 [==============================] - 14s 270us/step - loss: 0.1473 - acc: 0.9509 - val_loss: 1.2162 - val_acc: 0.7251\n",
            "Epoch 16/1000\n",
            "50000/50000 [==============================] - 14s 276us/step - loss: 0.1287 - acc: 0.9565 - val_loss: 1.3134 - val_acc: 0.7191\n",
            "Epoch 17/1000\n",
            "50000/50000 [==============================] - 14s 275us/step - loss: 0.1270 - acc: 0.9568 - val_loss: 1.2235 - val_acc: 0.7394\n",
            "Epoch 18/1000\n",
            "50000/50000 [==============================] - 14s 271us/step - loss: 0.1148 - acc: 0.9606 - val_loss: 1.2279 - val_acc: 0.7378\n",
            "Epoch 19/1000\n",
            "50000/50000 [==============================] - 14s 276us/step - loss: 0.1008 - acc: 0.9662 - val_loss: 1.3908 - val_acc: 0.7097\n",
            "Epoch 20/1000\n",
            "50000/50000 [==============================] - 14s 278us/step - loss: 0.1081 - acc: 0.9645 - val_loss: 1.4207 - val_acc: 0.7130\n",
            "Epoch 21/1000\n",
            "50000/50000 [==============================] - 14s 277us/step - loss: 0.0930 - acc: 0.9680 - val_loss: 1.5183 - val_acc: 0.7013\n",
            "Epoch 22/1000\n",
            "50000/50000 [==============================] - 14s 277us/step - loss: 0.1020 - acc: 0.9657 - val_loss: 1.8846 - val_acc: 0.6530\n",
            "Epoch 23/1000\n",
            "50000/50000 [==============================] - 14s 276us/step - loss: 0.0917 - acc: 0.9701 - val_loss: 1.3583 - val_acc: 0.7314\n",
            "Epoch 24/1000\n",
            "50000/50000 [==============================] - 14s 277us/step - loss: 0.0917 - acc: 0.9700 - val_loss: 1.4387 - val_acc: 0.7198\n",
            "Epoch 25/1000\n",
            "50000/50000 [==============================] - 14s 277us/step - loss: 0.0816 - acc: 0.9725 - val_loss: 1.3925 - val_acc: 0.7314\n",
            "Epoch 26/1000\n",
            "50000/50000 [==============================] - 14s 277us/step - loss: 0.0724 - acc: 0.9755 - val_loss: 1.4737 - val_acc: 0.7229\n",
            "Epoch 27/1000\n",
            "50000/50000 [==============================] - 14s 276us/step - loss: 0.0806 - acc: 0.9730 - val_loss: 1.6179 - val_acc: 0.6978\n",
            "Epoch 28/1000\n",
            "32896/50000 [==================>...........] - ETA: 4s - loss: 0.0781 - acc: 0.9739"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-2466fbdb08fc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     63\u001b[0m                     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m                     \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m                     validation_data=(x_test, y_test))\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;31m# Test the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "WS09ygw93YIL",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
